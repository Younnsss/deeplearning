# Fichier de configuration générique (à adapter au projet)
dataset:
  name: "tiny-imagenet" # ex: "CIFAR10", "IMDB", ...
  root: "./data" # chemin local pour les données
  split: # optionnel : noms/tailles de splits
    train: "train"
    val: "valid"
    test: null
  download: true # si supporté
  num_workers: 2
  shuffle: true

preprocess:
  # transformations de base (ex: resize, normalize)
  img_size: 64
  resize: [64, 64] # taille de redimensionnement
  normalize:
    mean: [0.485, 0.456, 0.406] # Valeurs ImageNet
    std: [0.229, 0.224, 0.225]

augment:
  # data augmentation pour l'entraînement
  random_horizontal_flip:
    p: 0.5
  random_rotation:
    degrees: 15
  random_resized_crop:
    scale: [0.8, 1.0]
    ratio: [0.9, 1.1]
  color_jitter:
    brightness: 0.1
    contrast: 0.1
    saturation: 0.1
    hue: 0.05
  random_erasing:
    enabled: false # optionnel, pas utilisé actuellement
    p: 0.2

model:
  type: "dilated_cnn" # 3-stage dilated CNN
  num_classes: 200 # Tiny ImageNet has 200 classes
  input_shape: [3, 64, 64] # 3 channels, 64x64 images

  # Model-specific parameters
  blocks_per_stage: [3, 3, 3] # (B1, B2, B3) - selected configuration
  dilations: [1, 2, 3] # (D1, D2, D3) - D1 always 1, selected (D2,D3) = (2,3)
  channels: [64, 128, 256] # Channels per stage

  # Standard parameters (keeping existing structure)
  activation: relu
  dropout: 0.0
  batch_norm: true # Required for this model
  residual: false
  attention: false

  # Unused for this model but kept for compatibility
  hidden_sizes: null
  rnn:
    type: null
    hidden_size: null
    num_layers: null
    bidirectional: false

train:
  seed: 42
  device: auto # "cpu", "cuda", ou "auto"
  batch_size: 64
  epochs: 15 # Set to middle of 10-20 range
  max_steps: null # entier ou null
  overfit_small: false # true pour sur-apprendre sur un petit échantillon

  optimizer:
    name: adam # sgd/adam/rmsprop
    lr: 0.0003 # Selected LR configuration
    weight_decay: 0.00001 # Selected weight decay configuration
    momentum: 0.9 # utile si SGD

  scheduler:
    name: none # none/step/cosine/onecycle
    step_size: 10
    gamma: 0.1
    warmup_steps: 0

metrics:
  classification: # ex: ["accuracy", "f1"]
    - accuracy
  regression: [] # ex: ["mae", "rmse"]

hparams: # espace pour mini grid search
  lr: [0.00025, 0.0005, 0.001] # Mini-grille ×0.5 ×1 ×2
  weight_decay: [0.00001, 0.0001] # Weight decay demandé
  blocks_per_stage: [[2, 2, 2], [3, 3, 3]] # (B1,B2,B3)
  dilations: [[2, 2], [2, 3]] # (D2,D3) - D1 always 1

paths:
  runs_dir: "./runs"
  artifacts_dir: "./artifacts"
